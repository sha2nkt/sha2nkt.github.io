<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shashank Tripathi - Computer Vision Researcher</title>
    <meta name="description" content="Shashank Tripathi - PhD Student at Max Planck Institute, Computer Vision and Human Motion Understanding">
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;500;700&display=swap" rel="stylesheet">
    
    <!-- FontAwesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <style type="text/css">
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #34495e;
            --accent-color: #3498db;
            --text-color: #2c3e50;
            --text-light: #7f8c8d;
            --background: #ffffff;
            --background-light: #f8f9fa;
            --border-color: #e9ecef;
            --shadow: 0 2px 10px rgba(0,0,0,0.08);
            --shadow-hover: 0 4px 20px rgba(0,0,0,0.12);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background: var(--background);
            font-size: 16px;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 30px;
        }

        /* Header */
        .header {
            background: linear-gradient(135deg, var(--background) 0%, var(--background-light) 100%);
            padding: 50px 0 35px;
            text-align: center;
            border-bottom: 1px solid var(--border-color);
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: radial-gradient(circle at 30% 70%, rgba(52, 152, 219, 0.05) 0%, transparent 50%);
            pointer-events: none;
        }

        .header .container {
            position: relative;
            z-index: 1;
        }

        .name {
            font-family: 'Playfair Display', serif;
            font-size: 3rem;
            font-weight: 700;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--accent-color) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 12px;
            letter-spacing: -0.5px;
            position: relative;
            animation: fadeInUp 0.4s ease-out;
        }

        .tagline {
            font-size: 1.1rem;
            color: var(--text-light);
            font-weight: 500;
            margin-bottom: 25px;
            position: relative;
            animation: fadeInUp 0.4s ease-out 0.05s both;
        }

        .tagline::after {
            content: '';
            display: block;
            width: 40px;
            height: 2px;
            background: linear-gradient(90deg, var(--accent-color), transparent);
            margin: 10px auto 0;
            animation: slideIn 0.4s ease-out 0.2s both;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes slideIn {
            from {
                width: 0;
            }
            to {
                width: 60px;
            }
        }

        /* Navigation */
        .nav-links {
            display: flex;
            justify-content: center;
            gap: 20px;
            list-style: none;
            margin-bottom: 30px;
            flex-wrap: wrap;
            animation: fadeInUp 0.5s ease-out 0.2s both;
        }

        .nav-link {
            color: var(--text-color);
            text-decoration: none;
            font-weight: 600;
            padding: 12px 20px;
            border-radius: 25px;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            font-size: 0.95rem;
            position: relative;
            background: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.2);
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            transform: translateY(0);
        }

        .nav-link::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(135deg, var(--accent-color), #2980b9);
            border-radius: 25px;
            opacity: 0;
            transition: opacity 0.3s ease;
            z-index: -1;
        }

        .nav-link:hover {
            color: white;
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(52, 152, 219, 0.3);
        }

        .nav-link:hover::before {
            opacity: 1;
        }

        .nav-link:hover i {
            transform: scale(1.1);
        }

        /* Social Links */
        .social-links {
            display: flex;
            justify-content: center;
            gap: 18px;
            margin-top: 35px;
            flex-wrap: wrap;
            animation: fadeInUp 0.5s ease-out 0.3s both;
        }

        .social-link {
            color: var(--text-light);
            text-decoration: none;
            padding: 12px 22px;
            border: 1px solid var(--border-color);
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 500;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            background: rgba(255, 255, 255, 0.6);
            backdrop-filter: blur(10px);
            overflow: hidden;
        }

        .social-link::before {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.3), transparent);
            transition: left 0.4s ease;
        }

        .social-link:hover::before {
            left: 100%;
        }

        .social-link:hover {
            background: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(52, 152, 219, 0.3);
        }

        .social-link:hover i {
            transform: scale(1.1);
        }

        /* Icon Styles */
        .nav-link i, .social-link i {
            margin-right: 8px;
            font-size: 0.9em;
            transition: all 0.3s ease;
            display: inline-block;
        }
        
        .social-link i {
            margin-right: 10px;
            transition: all 0.4s ease;
            opacity: 0.8;
        }

        /* Colorful Social Icons - Brand Colors */
        .social-link .fa-envelope {
            color: #ea4335;
        }
        
        .social-link .fa-graduation-cap {
            color: #4285f4;
        }
        
        .social-link .fa-github {
            color: #333;
        }
        
        .social-link .fa-linkedin {
            color: #0077b5;
        }
        
        .social-link .fa-twitter {
            color: #1da1f2;
        }
        
        .social-link .fa-file-pdf {
            color: #dc3545;
        }

        /* Enhanced hover effects for social icons */
        .social-link:hover i {
            opacity: 1;
            filter: brightness(1.1);
            transform: scale(1.1);
        }

        /* Keep social link text subtle while icons are colorful */
        .social-link {
            color: #6b7280;
        }

        .social-link:hover {
            color: white;
        }

        /* Floating Animation for Icons */
        .nav-link i {
            animation: float 2s ease-in-out infinite;
        }

        .nav-link:nth-child(1) i { animation-delay: 0s; }
        .nav-link:nth-child(2) i { animation-delay: 0.1s; }
        .nav-link:nth-child(3) i { animation-delay: 0.2s; }
        .nav-link:nth-child(4) i { animation-delay: 0.3s; }
        .nav-link:nth-child(5) i { animation-delay: 0.4s; }

        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-3px); }
        }

        /* Main Content */
        .main-content {
            padding: 60px 0;
        }

        .profile-section {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 50px;
            align-items: start;
            margin-bottom: 60px;
        }

        .profile-text {
            font-size: 1.05rem;
            line-height: 1.7;
        }

        .profile-text p {
            margin-bottom: 20px;
        }

        .profile-text a {
            color: var(--accent-color);
            text-decoration: none;
            font-weight: 500;
        }

        .profile-text a:hover {
            text-decoration: underline;
        }

        .profile-image {
            text-align: center;
        }

        .profile-image img {
            width: 250px;
            height: 250px;
            border-radius: 50%;
            object-fit: cover;
            box-shadow: var(--shadow);
        }

        /* Section Titles */
        .section-title {
            font-family: 'Playfair Display', serif;
            font-size: 2.2rem;
            color: var(--primary-color);
            margin-bottom: 40px;
            font-weight: 500;
        }

        /* Research Section */
        .research-section {
            background: var(--background-light);
            padding: 50px 0;
            margin: 60px 0;
            border-radius: 10px;
        }

        .research-content {
            max-width: 800px;
            margin: 0 auto;
            font-size: 1.05rem;
            line-height: 1.7;
            text-align: center;
        }

        /* Work Experience Section */
        .experience-logos {
            display: flex;
            justify-content: flex-start;
            align-items: center;
            gap: 60px;
            flex-wrap: wrap;
            margin-top: 30px;
            margin-bottom: 10px;
        }

        .experience-logo {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .experience-logo img {
            height: 80px;
            width: auto;
            max-width: 180px;
            object-fit: contain;
        }

        .experience-text {
            font-size: 0.8rem;
            font-weight: 600;
            color: var(--text-color);
            opacity: 0.7;
            transition: opacity 0.3s ease;
            text-align: center;
        }

        .experience-logo:hover .experience-text {
            opacity: 1;
        }

        /* Publications */
        .publication {
            display: grid;
            grid-template-columns: 220px 1fr;
            gap: 25px;
            margin-bottom: 15px;
            padding: 20px;
            background: white;
            border-radius: 8px;
            box-shadow: var(--shadow);
            transition: all 0.3s ease;
        }

        .publication:hover {
            box-shadow: var(--shadow-hover);
            transform: translateY(-2px);
        }

        .publication-image img {
            width: 100%;
            max-height: 180px;
            object-fit: contain;
            border-radius: 6px;
            /* background: var(--background-light); */
        }

        .publication-content h3 {
            font-family: 'Playfair Display', serif;
            font-size: 1.2rem;
            margin-bottom: 8px;
            color: var(--primary-color);
            font-weight: 500;
            line-height: 1.3;
        }

        .publication-content h3 a {
            color: inherit;
            text-decoration: none;
        }

        .publication-content h3 a:hover {
            color: var(--accent-color);
        }

        .authors {
            color: var(--text-light);
            margin-bottom: 6px;
            font-size: 0.9rem;
            line-height: 1.4;
        }

        .venue {
            color: var(--accent-color);
            font-weight: 500;
            margin-bottom: 8px;
            font-size: 0.9rem;
        }

        .award {
            color: #e74c3c;
            font-weight: 500;
            margin-bottom: 8px;
            font-size: 0.85rem;
        }

        .description {
            margin-bottom: 12px;
            line-height: 1.5;
            font-size: 0.9rem;
        }

        .paper-links {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .paper-link {
            padding: 4px 10px;
            background: var(--background-light);
            color: var(--text-color);
            text-decoration: none;
            border-radius: 12px;
            font-size: 0.8rem;
            font-weight: 500;
            transition: all 0.3s ease;
            border: 1px solid var(--border-color);
        }

        .paper-link:hover {
            background: var(--accent-color);
            color: white;
            border-color: var(--accent-color);
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 0 20px;
            }
            
            .name {
                font-size: 2.5rem;
                letter-spacing: -0.5px;
            }
            
            .tagline {
                font-size: 1rem;
            }
            
            .profile-section {
                grid-template-columns: 1fr;
                gap: 30px;
                text-align: center;
            }
            
            .publication {
                grid-template-columns: 1fr;
                gap: 15px;
            }
            
            .publication-image img {
                max-height: 120px;
                object-fit: contain;
            }
            
            .nav-links {
                gap: 12px;
                margin-bottom: 25px;
            }
            
            .nav-link {
                padding: 10px 16px;
                font-size: 0.9rem;
            }
            
            .social-links {
                gap: 14px;
                margin-top: 25px;
            }
            
            .social-link {
                padding: 10px 16px;
                font-size: 0.85rem;
            }
            
            .header {
                padding: 60px 0 40px;
            }

            .experience-logos {
                gap: 40px;
                margin-top: 25px;
                justify-content: center;
            }

            .experience-logo img {
                height: 60px;
                max-width: 140px;
            }
        }

        /* Abstract and BibTeX styles */
        .abstract {
            background: var(--background-light);
            padding: 20px;
            border-radius: 6px;
            margin: 15px 0;
            font-style: italic;
            line-height: 1.6;
            font-size: 0.9rem;
        }

        .bib {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            margin: 15px 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.8rem;
            border-left: 3px solid var(--accent-color);
            overflow-x: auto;
        }

        /* News Section */
        .news-section {
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            padding: 20px 0;
            margin: 30px 0;
            border-radius: 12px;
            border: 1px solid var(--border-color);
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }

        .news-content {
            max-width: 700px;
            margin: 0 auto;
        }

        .news-list {
            list-style: none;
            padding: 0;
            margin: 0;
            max-height: 180px; /* Height for approximately 3 items */
            overflow-y: auto;
            border-radius: 6px;
            border: 1px solid rgba(52, 152, 219, 0.1);
            padding: 10px;
        }

        /* Custom scrollbar styling */
        .news-list::-webkit-scrollbar {
            width: 6px;
        }

        .news-list::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 3px;
        }

        .news-list::-webkit-scrollbar-thumb {
            background: var(--accent-color);
            border-radius: 3px;
        }

        .news-list::-webkit-scrollbar-thumb:hover {
            background: #2980b9;
        }

        .news-item {
            padding: 6px 0;
            border-bottom: 1px solid rgba(52, 152, 219, 0.1);
            font-size: 0.9rem;
            line-height: 1.4;
            display: flex;
            align-items: flex-start;
            gap: 0;
        }

        .news-item:last-child {
            border-bottom: none;
            padding-bottom: 0;
        }

        .news-item:first-child {
            padding-top: 0;
        }

        .news-date {
            font-weight: 600;
            color: var(--accent-color);
            margin-right: 10px;
            min-width: 80px;
            flex-shrink: 0;
        }

        .news-text {
            color: var(--text-color);
            flex: 1;
        }

        .news-icon {
            /* color: var(--accent-color); */
            margin-right: 10px;
            font-size: 1.1em;
            vertical-align: middle;
        }

        /* Scroll to Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            width: 50px;
            height: 50px;
            background: linear-gradient(135deg, var(--accent-color), #2980b9);
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            font-size: 1.2rem;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 15px rgba(52, 152, 219, 0.3);
            transition: all 0.3s ease;
            z-index: 1000;
            opacity: 1;
            transform: translateY(0);
        }

        .scroll-to-top:hover {
            background: linear-gradient(135deg, #2980b9, var(--accent-color));
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(52, 152, 219, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        .scroll-to-top i {
            transition: transform 0.3s ease;
        }

        .scroll-to-top:hover i {
            transform: translateY(-2px);
        }

        /* Responsive adjustments for mobile */
        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 20px;
                right: 20px;
                width: 45px;
                height: 45px;
                font-size: 1.1rem;
            }
        }

        /* Job Market Notification Bar */
        .job-market-notification {
            background: linear-gradient(45deg, #667eea, #764ba2, #f093fb, #f5576c);
            background-size: 400% 400%;
            color: white;
            text-align: center;
            padding: 12px 18px;
            cursor: pointer;
            position: relative;
            border-bottom: 1px solid rgba(0,0,0,0.1);
            font-weight: 500;
            font-size: 1rem;
            transition: all 0.3s ease;
            animation: gradientShift 4s ease infinite;
            overflow: hidden;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.2);
            max-width: 100%;
        }

        .job-market-notification::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: linear-gradient(45deg, transparent, rgba(255,255,255,0.1), transparent);
            animation: shimmer 3s linear infinite;
            transform: rotate(45deg);
        }

        .job-market-notification::after {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(90deg, 
                transparent 0%, 
                rgba(255,255,255,0.05) 20%, 
                rgba(255,255,255,0.1) 50%, 
                rgba(255,255,255,0.05) 80%, 
                transparent 100%);
            animation: wave 2s ease-in-out infinite;
            pointer-events: none;
        }

        .job-market-notification:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
            animation-duration: 2s;
        }

        .job-market-notification i {
            margin-right: 8px;
            animation: gentlePulse 2s ease-in-out infinite;
            position: relative;
            z-index: 1;
        }

        .job-market-notification span {
            position: relative;
            z-index: 1;
        }

        @keyframes gradientShift {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        @keyframes shimmer {
            0% { transform: translateX(-100%) translateY(-100%) rotate(45deg); }
            100% { transform: translateX(100%) translateY(100%) rotate(45deg); }
        }

        @keyframes wave {
            0%, 100% { transform: translateX(-100%); }
            50% { transform: translateX(100%); }
        }

        @keyframes gentlePulse {
            0%, 100% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.05); opacity: 0.9; }
        }

        /* Toast notification for feedback */
        .toast {
            position: fixed;
            top: 20px;
            right: 20px;
            background: #27ae60;
            color: white;
            padding: 12px 20px;
            border-radius: 8px;
            font-weight: 500;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
            z-index: 1000;
            opacity: 0;
            transform: translateX(100%);
            transition: all 0.3s ease;
        }

        .toast.show {
            opacity: 1;
            transform: translateX(0);
        }

        @media (max-width: 768px) {
            .job-market-notification {
                padding: 10px 15px;
                font-size: 0.9rem;
                margin-bottom: 18px;
            }

            .toast {
                top: 10px;
                right: 10px;
                left: 10px;
                font-size: 0.9rem;
            }
        }

        /* Google Analytics */
    </style>
    
    <script type="text/javascript" src="hidebib.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-90857215-1', 'auto');
        ga('send', 'pageview');
    </script>
    
    <script>
        // Function to copy email to clipboard and show feedback
        function copyEmail() {
            const email = 'shashank.tripathi123@gmail.com';
            
            // Try to use the modern clipboard API
            if (navigator.clipboard && navigator.clipboard.writeText) {
                navigator.clipboard.writeText(email).then(function() {
                    showToast('Email copied to clipboard!');
                }).catch(function(err) {
                    // Fallback for older browsers
                    fallbackCopyText(email);
                });
            } else {
                // Fallback for older browsers
                fallbackCopyText(email);
            }
        }
        
        // Fallback method for copying text
        function fallbackCopyText(text) {
            const textArea = document.createElement('textarea');
            textArea.value = text;
            textArea.style.position = 'fixed';
            textArea.style.left = '-999999px';
            textArea.style.top = '-999999px';
            document.body.appendChild(textArea);
            textArea.focus();
            textArea.select();
            
            try {
                document.execCommand('copy');
                showToast('Email copied to clipboard!');
            } catch (err) {
                showToast('Failed to copy email. Please copy manually: ' + text);
            }
            
            document.body.removeChild(textArea);
        }
        
        // Function to show toast notification
        function showToast(message) {
            // Remove any existing toast
            const existingToast = document.querySelector('.toast');
            if (existingToast) {
                existingToast.remove();
            }
            
            // Create new toast
            const toast = document.createElement('div');
            toast.className = 'toast';
            toast.textContent = message;
            document.body.appendChild(toast);
            
            // Show toast with animation
            setTimeout(() => {
                toast.classList.add('show');
            }, 100);
            
            // Hide toast after 3 seconds
            setTimeout(() => {
                toast.classList.remove('show');
                setTimeout(() => {
                    if (toast.parentNode) {
                        toast.parentNode.removeChild(toast);
                    }
                }, 300);
            }, 3000);
        }
    </script>
</head>

<body>
    <!-- Header Section -->
    <header class="header">
        <div class="container">
            <h1 class="name">Shashank Tripathi</h1>
            <p class="tagline">PhD Student • Computer Vision</p>
            
            <!-- Navigation -->
            <ul class="nav-links">
                <li><a href="#about" class="nav-link"><i class="fas fa-user"></i>About</a></li>
                <li><a href="#research" class="nav-link"><i class="fas fa-microscope"></i>Research</a></li>
                <li><a href="#publications" class="nav-link"><i class="fas fa-file-alt"></i>Publications</a></li>
                <li><a href="#patents" class="nav-link"><i class="fas fa-lightbulb"></i>Patents</a></li>
                <li><a href="#misc" class="nav-link"><i class="fas fa-ellipsis-h"></i>Misc</a></li>
            </ul>
            
            <!-- Social Links -->
            <div class="social-links">
                <a href="mailto:shashank.tripathi123@gmail.com" class="social-link"><i class="fas fa-envelope"></i>Email</a>
                <a href="https://scholar.google.com/citations?user=CANstcsAAAAJ&hl=en" class="social-link"><i class="fas fa-graduation-cap"></i>Google Scholar</a>
                <a href="https://github.com/sha2nkt" class="social-link"><i class="fab fa-github"></i>GitHub</a>
                <a href="https://www.linkedin.com/in/shashanktripathi123/" class="social-link"><i class="fab fa-linkedin"></i>LinkedIn</a>
                <a href="https://twitter.com/sha2nk_t" class="social-link"><i class="fab fa-twitter"></i>Twitter</a>
                <a href="assets/CV_Shashank_LongPhD_V33.pdf" class="social-link"><i class="fas fa-file-pdf"></i>CV</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main class="main-content">
        <div class="container">
            <!-- About Section -->
            <section id="about" class="profile-section">
                <div class="profile-text">
                    <!-- Job Market Notification Bar -->
                    <div class="job-market-notification" onclick="copyEmail()" title="Click to copy email address" style="margin-bottom: 20px; border-radius: 12px;">
                        <i class="fas fa-briefcase"></i>
                        <span>I am currently on the job market. Feel free to connect!</span>
                    </div>
                    <p>I am a PhD student (2021-) at the <strong>Max Planck Institute for Intelligent Systems</strong> where I am advised by MPI Director <a href="https://ps.is.mpg.de/person/black">Michael Black</a>. Earlier, I worked as an Applied Scientist at Amazon (2019-2021). I earned my Masters (2017-2019) from the Robotics Institute, Carnegie Mellon University, working with Prof. <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>. I am a recipient of the <a href="https://research.facebook.com/blog/2023/4/announcing-the-2023-meta-research-phd-fellowship-award-winners/">Meta Research PhD Fellowship award</a> in 2023.</p>

                    <p>At Amazon Lab126, I closely collaborated with Prof. <a href="https://rehg.org/">James Rehg</a>, Dr. <a href="https://scholar.google.com/citations?user=Q3puGtcAAAAJ&hl=en">Amit Agrawal</a> and Dr. <a href="https://scholar.google.com/citations?user=GaSWCoUAAAAJ&hl=en">Ambrish Tyagi</a>. In 2023, I spent time at <a href="https://www.unrealengine.com/en-US/">Epic Games</a> as a research intern working with Dr. <a href="https://www.linkedin.com/in/carstenstoll/">Carsten Stoll</a>, Dr. <a href="https://christophlassner.de/">Christoph Lassner</a> and Dr. <a href="https://theorangeduck.com/">Daniel Holden</a>. Recently, I interned at Meta Zurich where I worked with Dr. <a href="https://btekin.github.io/">Bugra Tekin</a> on impoving spatial understanding and visual grounding in 3D foundation models (VLMs).</p> 
                        
                    <p>It has been my great fortune to have worked with excellent mentors and advisors.</p>
                    
                    <!-- Work Experience Section -->
                    <h3 style="margin-top: 30px; margin-bottom: 20px; color: var(--primary-color); font-family: 'Playfair Display', serif; font-size: 1.5rem; font-weight: 500;">Work Experience</h3>
                    <div class="experience-logos">
                        <div class="experience-logo">
                            <img src="assets/Amazon_logo.png" alt="Amazon" title="Amazon Lab126 - Applied Scientist (2019-2021)">
                        </div>
                        <div class="experience-logo">
                            <img src="assets/Epic_Games_logo.png" alt="Epic Games" title="Epic Games - Research Intern (2023)">
                        </div>
                        <div class="experience-logo">
                            <img src="assets/Meta-Logo.png" alt="Meta" title="Meta Zurich - Research Intern (2024)">
                        </div>
                    </div>
                </div>
                
                <div class="profile-image">
                    <img src="assets/shashank_2.png" alt="Shashank Tripathi">
                </div>
            </section>

            <!-- Recent News Section -->
            <section id="news" class="news-section">
                <div class="container">
                    <h2 class="section-title">
                        <i class="fas fa-newspaper news-icon"></i>
                        Recent News
                    </h2>
                    <div class="news-content">
                        <ul class="news-list">
                            <li class="news-item">
                                <span class="news-date">Oct 2025:</span>
                                <span class="news-text">🎉 1 paper accepted in ICCV 2025 - <a href="https://anticdimi.github.io/sdfit/">SDFit</a></span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Jun 2025:</span>
                                <span class="news-text">🏆 Selected for the <a href="https://cvpr.thecvf.com/Conferences/2025/CallForDoctoralConsortium">Doctoral Consortium</a> at CVPR 2025</span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Jun 2025:</span>
                                <span class="news-text">🏆 <a href="https://interactvlm.is.tue.mpg.de/">InteractVLM</a> won the Human Contact Estimation Challenge at CVPR 2025</span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Jun 2025:</span>
                                <span class="news-text">🏆 Received the <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer">Outstanding Reviewer Award</a> for CVPR 2025</span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Jun 2025:</span>
                                <span class="news-text">🎉 2 papers accepted in CVPR 2025 - <a href="https://pico.is.tue.mpg.de/">PICO</a>, <a href="https://interactvlm.is.tue.mpg.de/">InteractVLM</a></span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Jun 2025:</span>
                                <span class="news-text">🎤 Organized two workshops at CVPR 2025: <a href="https://sites.google.com/view/3d-humans-cvpr2025">3D HUMANS</a>, <a href="https://rhobin-challenge.github.io/">RHOBIN</a></span>
                            </li>
                            <li class="news-item">
                                <span class="news-date">Oct 2024:</span>
                                <span class="news-text">💼 Started internship at Meta Zurich</span>
                            </li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Research Section -->
            <section id="research" class="research-section">
                <div class="container">
                    <h2 class="section-title">Research</h2>
                    <div class="research-content">
                        <p>My research lies at the intersection of <strong>machine learning</strong>, <strong>computer vision</strong> and <strong>computer graphics</strong>. Specifically, I am interested in 3D modeling of human bodies, modeling human-object interactions, physics-inspired human motion understanding and spatial understanding of 3D scenes. In the past, I have worked on synthetic data for applications like object detection and human pose estimation from limited supervision.</p>
                        
                        <!-- <p>Before diving into human body research, I dabbled in visual-servoing, medical-image analysis, pedestrian-detection and reinforcement learning.</p> -->
                    </div>
                </div>
            </section>

            <!-- Publications Section -->
            <section id="publications">
                <h2 class="section-title">Publications</h2>

                <!-- SDFit Publication -->
                <article class="publication">
                    <div class="publication-image">
                        <a href="assets/sdfit_thumb2.png">
                            <img src="assets/sdfit_thumb2.png" alt="SDFit Teaser">
                        </a>
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://anticdimi.github.io/sdfit/">SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single Image</a></h3>
                        <div class="authors">Dimitrije Antić, Georgios Paschalidis, <strong>Shashank Tripathi</strong>, Theo Gevers, Sai Kumar Dwivedi, Dimitrios Tzionas</div>
                        <div class="venue">IEEE/CVF International Conference on Computer Vision (ICCV) 2025</div>
                        <div class="description">SDFit is a novel optimization framework that uses a morphable signed-distance-function prior and 2D–3D correspondences from foundational models to iteratively recover and refine 3D object shape and pose from a single image</div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2409.16178" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('sdfit_abs')" class="paper-link">Abstract</a>
                            <a href="https://anticdimi.github.io/sdfit/" class="paper-link">Project</a>
                            <!-- <a href="https://interactvlm.is.tue.mpg.de/media/upload/InteractVLM_Poster.pdf" class="paper-link">Poster</a> -->
                            <!-- <a href="https://www.youtube.com/watch?v=brxygxM1nRk" class="paper-link">Video</a> -->
                            <a href="javascript:togglebib('sdfit')" class="paper-link">BibTex</a>
                        </div>
                        <div class="abstract" id="sdfit_abs" style="display: none;">
                            Recovering 3D object pose and shape from a single image is a challenging and highly ill-posed problem. This is due to strong (self-)occlusions, depth ambiguities, the vast intra- and inter-class shape variance, and lack of 3D ground truth for natural images. While existing methods train deep networks on synthetic datasets to predict 3D shapes, they often struggle to generalize to real-world scenarios, lack an explicit feedback loop for refining noisy estimates, and primarily focus on geometry without explicitly considering pixel alignment. To this end, we make two key observations: (1) a robust solution requires a model that imposes a strong category-specific shape prior to constrain the search space, and (2) foundational models embed 2D images and 3D shapes in joint spaces; both help resolve ambiguities. Hence, we propose SDFit, a novel optimization framework that is built on three key innovations: First, we use a learned morphable signed-distance-function (mSDF) model that acts as a strong shape prior, thus constraining the shape space. Second, we use foundational models to establish rich 2D-to-3D correspondences between image features and the mSDF. Third, we develop a fitting pipeline that iteratively refines both shape and pose, aligning the mSDF to the image. We evaluate SDFit on the Pix3D, Pascal3D+, and COMIC image datasets. SDFit performs on par with SotA methods, while demonstrating exceptional robustness to occlusions and requiring no retraining for unseen images. Therefore, SDFit contributes new insights for generalizing in the wild, paving the way for future research. Code will be released.
                        </div>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="sdfit">
@inproceedings{antic2025sdfit,
  title     = {{SDFit}: {3D} Object Pose and Shape by Fitting a Morphable {SDF} to a Single Image},
  author    = {Anti\'{c}, Dimitrije and Paschalidis, Georgios and Tripathi, Shashank and Gevers, Theo and Dwivedi, Sai Kumar and Tzionas, Dimitrios},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2025},
}
                        </div>
                    </div>
                </article>

                <!-- InteractVLM Publication -->
                <article class="publication">
                    <div class="publication-image">
                        <a href="assets/interactvlm_teaser.png">
                            <img src="assets/interactvlm_teaser.png" alt="InteractVLM Teaser">
                        </a>
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://interactvlm.is.tue.mpg.de/">InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</a></h3>
                        <div class="authors">Sai Kumar Dwivedi, Dimitrije Antić, <strong>Shashank Tripathi</strong>, Omid Taheri, Cordelia Schmid, Michael J. Black, Dimitrios Tzionas</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2025</div>
                        <div class="award">(Winner of the contact estimation tracks at CVPR 2025 - <a href="https://rhobin-challenge.github.io/">see here</a>)</div>
                        <div class="description">InteractVLM is a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate joint reconstruction by leveraging large foundational model.</div>
                        <div class="paper-links">
                            <a href="https://interactvlm.is.tue.mpg.de/media/upload/paper.pdf" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('interactvlm_abs')" class="paper-link">Abstract</a>
                            <a href="https://interactvlm.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://interactvlm.is.tue.mpg.de/media/upload/InteractVLM_Poster.pdf" class="paper-link">Poster</a>
                            <a href="https://www.youtube.com/watch?v=brxygxM1nRk" class="paper-link">Video</a>
                            <a href="javascript:togglebib('interactvlm')" class="paper-link">BibTex</a>
                        </div>
                        <div class="abstract" id="interactvlm_abs" style="display: none;">
                            We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available.
                        </div>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="interactvlm">
@inproceedings{dwivedi_interactvlm_2025,
  title     = {{InteractVLM}: {3D} Interaction Reasoning from {2D} Foundational Models},
  author    = {Dwivedi, Sai Kumar and Antić, Dimitrije and Tripathi, Shashank and Taheri, Omid and Schmid, Cordelia and Black, Michael J. and Tzionas, Dimitrios},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2025},
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/pico_teaser.gif" alt="PICO: Reconstructing 3D People In Contact with Objects">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://pico.is.tue.mpg.de/" id="PICO">PICO: Reconstructing 3D People In Contact with Objects</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>*, Alpár Cseke*, Sai Kumar Dwivedi, Arjun Lakshmipathy, Agniv Chatterjee, Michael J. Black, Dimitrios Tzionas</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2025</div>
                        <div class="description">
                            PICO recovers humans, objects, and their interactions (HOI) - all in 3D, from just a single internet image. To this end, we collect a new dataset of 3D contact correspondences, called PICO-db and a novel three-stage optimization framework PICO-fit.
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Cseke_PICO_Reconstructing_3D_People_In_Contact_with_Objects_CVPR_2025_paper.html" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('pico_abs')" class="paper-link">Abstract</a>
                            <a href="https://pico.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://pico.is.tue.mpg.de/media/upload/static/images/CVPR2025_PICO_Poster.pdf" class="paper-link">Poster</a>
                            <a href="https://www.youtube.com/watch?v=w-LncKIjgSw" class="paper-link">Video</a>
                            <a href="javascript:togglebib('pico')" class="paper-link togglebib">BibTeX</a>
                            <a href="https://pico.is.tue.mpg.de/dataexploration.html" class="paper-link">Dataset</a>
                        </div>
                        <p style="text-align: justify;"><i id="pico_abs" style="display: none;">Recovering 3D Human-Object Interaction (HOI) from single color images is challenging due to depth ambiguities, occlusions, and the huge variation in object shape and appearance. Thus, past work requires controlled settings such as known object shapes and contacts, and tackles only limited object classes. Instead, we need methods that generalize to natural images and novel object classes. We tackle this in two main ways: (1) We collect PICO-db, a new dataset of natural images uniquely paired with dense 3D contact on both body and object meshes. To this end, we use images from the recent DAMON dataset that are paired with contacts, but these contacts are only annotated on a canonical 3D body. In contrast, we seek contact labels on both the body and the object. To infer these given an image, we retrieve an appropriate 3D object mesh from a database by leveraging vision foundation models. Then, we project DAMON's body contact patches onto the object via a novel method needing only 2 clicks per patch. This minimal human input establishes rich contact correspondences between bodies and objects. (2) We exploit our new dataset of contact correspondences in a novel render-and-compare fitting method, called PICO-fit, to recover 3D body and object meshes in interaction. PICO-fit infers contact for the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db for that object, and uses the contact to iteratively fit the 3D body and object meshes to image evidence via optimization. Uniquely, PICO-fit works well for many object categories that no existing method can tackle. This is crucial to enable HOI understanding to scale in the wild. Our data and code are available.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="pico">
@inproceedings{cseke_tripathi_2025_pico,
title = {{PICO}: Reconstructing {3D} People In Contact with Objects},
author = {Cseke, Alp\'{a}r and Tripathi, Shashank and Dwivedi, Sai Kumar and
Lakshmipathy, Arjun and Chatterjee, Agniv and Black, Michael J. and Tzionas,
Dimitrios},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)},
month = {June},
year = {2025},
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/teaser_humos.gif" alt="HUMOS: HUman MOtion Model Conditioned on Body Shape">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://carstenepic.github.io/humos/" id="HUMOS">HUMOS: HUman MOtion Model Conditioned on Body Shape</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Omid Taheri, Christoph Lassner, Michael J. Black, Daniel Holden, Carsten Stoll</div>
                        <div class="venue">European Conference on Computer Vision (ECCV) 2024</div>
                        <div class="description">
                            People with different body shapes perform the same motion differently. Our method, HUMOS, generates natural, physically plausible, and dynamically stable human motions based on body shape. 
                            <!-- HUMOS introduces a novel identity-preserving cycle consistency loss and uses differentiable dynamic stability and physics terms to learn an identity-conditioned manifold of human motions. -->
                        </div>
                        <div class="paper-links">
                            <a href="https://eccv.ecva.net/virtual/2024/poster/262" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('humos_abs')" class="paper-link">Abstract</a>
                            <a href="https://carstenepic.github.io/humos/" class="paper-link">Project</a>
                            <a href="https://www.youtube.com/watch?v=yLXX7TxBA4o" class="paper-link">Video</a>
                            <a href="javascript:togglebib('humos')" class="paper-link togglebib">BibTeX</a>
                            <a href="https://www.dropbox.com/scl/fi/nxtj4svwe5dcfvaffou0u/ECCV2024_HUMOS_Poster_v2.pdf?rlkey=3cku1bxgio9ec7o4bumetqiu7&st=un1ub1c9&dl=0" class="paper-link">Poster</a>
                        </div>
                        <p style="text-align: justify;"><i id="humos_abs" style="display: none;">Generating realistic human motion is an important task in many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically ignore these differences and use a normalized, average body size. This leads to a homogenization of motion across human bodies that limits diversity and that may not align with their physical attributes. We propose a novel approach to learn a generative motion model conditioned on body shape. We demonstrate that it is possible to learn such a model from unpaired training data using cycle consistency and intuitive physics and stability constraints that model the correlation between identity and movement. The resulting model produces diverse, physically plausible, dynamically stable human motions that are quantitatively and qualitatively more realistic than the existing state of the art.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="humos">
@inproceedings{tripathi2024humos,
title = {{HUMOS}: Human Motion Model Conditioned on Body Shape},
author = {Tripathi, Shashank and Taheri, Omid and Lassner, Christoph and
Black, Michael J. and Holden, Daniel and Stoll, Carsten},
booktitle = {European Conference on Computer Vision},
pages = {133--152},
year = {2025},
organization = {Springer}
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/teaser_deco_square.png" alt="DECO: Dense Estimation of 3D Human-Scene COntact in the Wild">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://deco.is.tue.mpg.de/" id="DECO">DECO: Dense Estimation of 3D Human-Scene COntact in the Wild</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>*, Agniv Chatterjee*, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, Michael J. Black</div>
                        <div class="venue">International Conference on Computer Vision (ICCV) 2023</div>
                        <div class="award">(Oral presentation)</div>
                        <div class="description">
                            DECO estimates dense vertex-level 3D human-scene and human-object contact across the full body mesh and works on diverse and challenging human-object interactions in arbitrary in-the-wild images. 
                            <!-- DECO is trained on DAMON, a new and unique dataset with 3D contact annotations for in-the-wild images, manually annotated using a custom 3D contact labeling tool. -->
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.html" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('deco_abs')" class="paper-link">Abstract</a>
                            <a href="https://deco.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://deco.is.tue.mpg.de/download.php" class="paper-link">Dataset</a>
                            <a href="https://www.youtube.com/watch?v=o7MLobqAFTQ&feature=youtu.be" class="paper-link">Video</a>
                            <a href="javascript:togglebib('deco')" class="paper-link togglebib">BibTeX</a>
                            <a href="https://www.dropbox.com/scl/fi/kvhpfnkvga2pt19ayko8u/ICCV2023_DECO_Poster_v2.pptx?rlkey=ihbf3fi6u9j0ha9x1gfk2cwd0&dl=0" class="paper-link">Poster</a>
                        </div>
                        <p style="text-align: justify;"><i id="deco_abs" style="display: none;">Understanding how humans use physical contact to interact with the world is key to enabling human-centric artificial intelligence. While inferring 3D contact is crucial for modeling realistic and physically-plausible human-object interactions, existing methods either focus on 2D, consider body joints rather than the surface, use coarse 3D body regions, or do not generalize to in-the-wild images. In contrast, we focus on inferring dense, 3D contact between the full body surface and objects in arbitrary images. To achieve this, we first collect DAMON, a new dataset containing dense vertex-level contact annotations paired with RGB images containing complex human-object and human-scene contact. Second, we train DECO, a novel 3D contact detector that uses both body-part-driven and scene-context-driven attention to estimate vertex-level contact on the SMPL body. DECO builds on the insight that human observers recognize contact by reasoning about the contacting body parts, their proximity to scene objects, and the surrounding scene context. We perform extensive evaluations of our detector on DAMON as well as on the RICH and BEHAVE datasets. We significantly outperform existing SOTA methods across all benchmarks. We also show qualitatively that DECO generalizes well to diverse and challenging real-world human interactions in natural images. The code, data, and models are available for research purposes.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="deco">
@inproceedings{tripathi2023deco,
title = {{DECO}: Dense Estimation of {3D} Human-Scene Contact In The Wild},
author = {Tripathi, Shashank and Chatterjee, Agniv and Passy, Jean-Claude
and Yi, Hongwei and Tzionas, Dimitrios and Black, Michael J.},
booktitle = {Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)},
month = {October},
year = {2023},
pages = {8001-8013}
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/teaser_emote.png" alt="EMOTE: Emotional Speech-Driven Animation with Content-Emotion Disentanglement">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://emote.is.tue.mpg.de/" id="EMOTE">EMOTE: Emotional Speech-Driven Animation with Content-Emotion Disentanglement</a></h3>
                        <div class="authors">Radek Danecek, Kiran Chhatre, <strong>Shashank Tripathi</strong>, Yandong Wen, Michael J. Black, Timo Bolkart</div>
                        <div class="venue">SIGGRAPH ASIA 2023</div>
                        <div class="description">
                            Given audio input and an emotion label, EMOTE generates an animated 3D head that has state-of-the-art lip synchronization while expressing the emotion. The method is trained from 2D video sequences using a novel video emotion loss and a mechanism to disentangle emotion from speech.
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2306.08990" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('emote_abs')" class="paper-link">Abstract</a>
                            <a href="https://emote.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="javascript:togglebib('emote')" class="paper-link togglebib">BibTeX</a>
                        </div>
                        <p style="text-align: justify;"><i id="emote_abs" style="display: none;">To be widely adopted, 3D facial avatars must be animated easily, realistically, and directly from speech signals. While the best recent methods generate 3D animations that are synchronized with the input audio, they largely ignore the impact of emotions on facial expressions. Realistic facial animation requires lip-sync together with the natural expression of emotion. To that end, we propose EMOTE (Expressive Model Optimized for Talking with Emotion), which generates 3D talking-head avatars that maintain lip-sync from speech while enabling explicit control over the expression of emotion. To achieve this, we supervise EMOTE with decoupled losses for speech (i.e., lip-sync) and emotion. These losses are based on two key observations: (1) deformations of the face due to speech are spatially localized around the mouth and have high temporal frequency, whereas (2) facial expressions may deform the whole face and occur over longer intervals. Thus, we train EMOTE with a per-frame lip-reading loss to preserve the speech-dependent content, while supervising emotion at the sequence level. Furthermore, we employ a content-emotion exchange mechanism in order to supervise different emotions on the same audio, while maintaining the lip motion synchronized with the speech. To employ deep perceptual losses without getting undesirable artifacts, we devise a motion prior in the form of a temporal VAE. Due to the absence of high-quality aligned emotional 3D face datasets with speech, EMOTE is trained with 3D pseudo-ground-truth extracted from an emotional video dataset (i.e., MEAD). Extensive qualitative and perceptual evaluations demonstrate that EMOTE produces speech-driven facial animations with better lip-sync than state-of-the-art methods trained on the same data, while offering additional, high-quality emotional control.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="emote">
@inproceedings{EMOTE,
title = {Emotional Speech-Driven Animation with Content-Emotion
Disentanglement},
author = {Danecek, Radek and Chhatre, Kiran and Tripathi, Shashank and Wen,
Yandong and Black, Michael and Bolkart, Timo},
publisher = {ACM},
year = {2023},
doi = {10.1145/3610548.3618183},
url = {https://emote.is.tue.mpg.de/index.html}
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/teaser_ipman_square.jpeg" alt="3D Human Pose Estimation via Intuitive Physics">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://ipman.is.tue.mpg.de/" id="IPMAN">3D Human Pose Estimation via Intuitive Physics</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Lea Müller, Chun-Hao P. Huang, Omid Taheri, Michael Black, Dimitrios Tzionas</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2023</div>
                        <div class="description">
                            IPMAN estimates a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. It exploits interpenetration of the body mesh with the ground plane as a heuristic for pressure.
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.html" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('ipman_abs')" class="paper-link">Abstract</a>
                            <a href="https://ipman.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://moyo.is.tue.mpg.de/" class="paper-link">Dataset</a>
                            <a href="https://www.youtube.com/watch?v=eZTtLUMnGIg" class="paper-link">Video</a>
                            <a href="javascript:togglebib('ipman')" class="paper-link togglebib">BibTeX</a>
                            <a href="https://drive.google.com/file/d/1n8QeOI_WRqcVDUMrB-lG2NCJURhBjppG/view?usp=sharing" class="paper-link">Poster</a>
                        </div>
                        <p style="text-align: justify;"><i id="ipman_abs" style="display: none;">The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a "stable" configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="ipman">
@inproceedings{tripathi2023ipman,
title = {{3D} Human Pose Estimation via Intuitive Physics},
author = {Tripathi, Shashank and M{\"u}ller, Lea and Huang, Chun-Hao P. and
Taheri Omid
and Black, Michael J. and Tzionas, Dimitrios},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern
Recognition (CVPR)},
month = {June},
year = {2023}
}
                        </div>
                    </div>
                </article>

                <!-- ...existing code... (next publication) -->

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/bite_teaser_2.gif" alt="BITE: Beyond Priors for Improved Three-Dog Pose Estimation">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://bite.is.tue.mpg.de/" id="BITE">BITE: Beyond Priors for Improved Three-Dog Pose Estimation</a></h3>
                        <div class="authors">Nadine Rüegg, <strong>Shashank Tripathi</strong>, Konrad Schindler, Michael J. Black, Silvia Zuffi</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2023</div>
                        <div class="description">
                            BITE enables 3D shape and pose estimation of dogs from a single input image. The model handles a wide range of shapes and breeds, as well as challenging postures far from the available training poses, like sitting or lying on the ground.
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ruegg_BITE_Beyond_Priors_for_Improved_Three-D_Dog_Pose_Estimation_CVPR_2023_paper.html" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('bite_abs')" class="paper-link">Abstract</a>
                            <a href="https://bite.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://www.youtube.com/watch?v=mOeLtNk070E" class="paper-link">Video</a>
                            <a href="javascript:togglebib('bite')" class="paper-link togglebib">BibTeX</a>
                            <a href="assets/poster_v5_final_print.pdf" class="paper-link">Poster</a>
                        </div>
                        <p style="text-align: justify;"><i id="bite_abs" style="display: none;">We address the problem of inferring the 3D shape and pose of dogs from images. Given the lack of 3D training data, this problem is challenging, and the best methods lag behind those designed to estimate human shape and pose. To make progress, we attack the problem from multiple sides at once. First, we need a good 3D shape prior, like those available for humans. To that end, we learn a dog-specific 3D parametric model, called D-SMAL. Second, existing methods focus on dogs in standing poses because when they sit or lie down, their legs are self occluded and their bodies deform. Without access to a good pose prior or 3D data, we need an alternative approach. To that end, we exploit contact with the ground as a form of side information. We consider an existing large dataset of dog images and label any 3D contact of the dog with the ground. We exploit body-ground contact in estimating dog pose and find that it significantly improves results. Third, we develop a novel neural network architecture to infer and exploit this contact information. Fourth, to make progress, we have to be able to measure it. Current evaluation metrics are based on 2D features like keypoints and silhouettes, which do not directly correlate with 3D errors. To address this, we create a synthetic dataset containing rendered images of scanned 3D dogs. With these advances, our method recovers significantly better dog shape and pose than the state of the art, and we evaluate this improvement in 3D. Our code, model and test dataset are publicly available for research purposes at https://bite.is.tue.mpg.de.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="bite">
@inproceedings{bite2023rueegg,
title = {{BITE}: Beyond Priors for Improved Three-{D} Dog Pose Estimation},
author = {R\"uegg, Nadine and Tripathi, Shashank and Schindler, Konrad and
Black, Michael J. and Zuffi, Silvia},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern
Recognition (CVPR)},
pages = {8867-8876},
month = {June},
year = {2023}
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/mime_teaser.gif" alt="MIME: Human-Aware 3D Scene Generation">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://mime.is.tue.mpg.de/" id="MIME">MIME: Human-Aware 3D Scene Generation</a></h3>
                        <div class="authors">Hongwei Yi, Chun-Hao P. Huang, <strong>Shashank Tripathi</strong>, Lea Hering, Justus Thies, Michael J. Black</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2023</div>
                        <div class="description">
                            MIME takes 3D human motion capture and generates plausible 3D scenes that are consistent with the motion. Why? Most mocap sessions capture the person but not the scene.
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.html" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('mime_abs')" class="paper-link">Abstract</a>
                            <a href="https://mime.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a href="https://drive.google.com/file/d/1LfaS9ijbTJZ4rDacJa3Syzljj1lRiYss/view?usp=sharing" class="paper-link">Video</a>
                            <a href="javascript:togglebib('mime')" class="paper-link togglebib">BibTeX</a>
                            <a href="assets/poster_v5_final_print.pdf" class="paper-link">Poster</a>
                        </div>
                        <p style="text-align: justify;"><i id="mime_abs" style="display: none;">Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a "scanner" of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research.</i></p>
                        <div style="white-space: pre-wrap; display: none;" class="bib" id="mime">
@inproceedings{yi2022mime,
title = {{MIME}: Human-Aware {3D} Scene Generation},
author = {Yi, Hongwei and Huang, Chun-Hao P. and Tripathi, Shashank and
Hering, Lea and Thies, Justus and Black, Michael J.},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern
Recognition (CVPR)},
pages={12965-12976},
month = {June},
year = {2023}
}
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/peri_arch3.png" alt="PERI: Part Aware Emotion Recognition in the Wild">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://link.springer.com/chapter/10.1007/978-3-031-25075-0_6" id="PERI">PERI: Part Aware Emotion Recognition in the Wild</a></h3>
                        <div class="authors">Akshita Mittel, <strong>Shashank Tripathi</strong></div>
                        <div class="venue">European Conference on Computer Vision Workshops (ECCVW) 2022</div>
                        <div class="description">
                            An in-the-wild emotion recognition network that leverages both body pose and facial landmarks using a novel part aware spatial (PAS) image representation and context infusion (Cont-In) blocks.
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2210.10130" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('peri_abs')" class="paper-link">Abstract</a>
                            <!-- <a href="https://rawalkhirodkar.github.io/ochmr">project</a> | -->
                            <!-- <a href="https://www.https://www.youtube.com/watch?v=Q_43QXR1pzY&t=42s">video</a> -->
                            <!-- <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> | -->
                            <!-- <a href="assets/poster_v5_final_print.pdf">poster</a> -->

                            <!-- <a href="https://sites.google.com/corp/view/actionabl
                                    between facial expression and the emotional state of a person,
                                    pioneering methods rely primarily on facial features. However, facial
                                    features are often unreliable in natural unconstrained scenarios, such
                                    as in crowded scenes, as the face lacks pixel resolution and contains
                                    artifacts due to occlusion and blur. To address this, in the wild
                                    emotion recognition exploits full-body person crops as well as the
                                    surrounding scene context. In a bid to use body pose for emotion
                                    recognition, such methods fail to realize the potential that facial
                                    expressions, when available, offer. Thus, the aim of this paper is
                                    two-fold. First, we demonstrate our method, PERI, to leverage both body
                                    pose and facial landmarks. We create part aware spatial (PAS) images by
                                    extracting key regions from the input image using a mask generated from
                                    both body pose and facial landmarks. This allows us to exploit body pose
                                    in addition to facial context whenever available. Second, to reason from
                                    the PAS images, we introduce context infusion (Cont-In) blocks. These
                                    blocks attend to part-specific information, and pass them onto the
                                    intermediate features of an emotion recognition network. Our approach is
                                    conceptually simple and can be applied to any existing emotion
                                    recognition method. We provide our results on the publicly available in
                                    the wild EMOTIC dataset. Compared to existing methods, PERI achieves
                                    superior performance and leads to significant improvements in the mAP of
                                    emotion categories, while decreasing Valence, Arousal and Dominance
                                    errors. Importantly, we observe that our method improves performance in
                                    both images with fully visible faces as well as in images with occluded
                                    or blurred faces. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{mitell2022peri,
title = {{PERI}: Part Aware Emotion Recognition in the Wild},
author = {Mittel, Akshita and Tripathi, Shashank},
booktitle="Computer Vision -- ECCV 2022 Workshops",
year = {2023},
publisher="Springer Nature Switzerland",
pages="76--92",
}
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/teaser_ochmr_square.png" alt="Occluded Human Mesh Recovery">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Khirodkar_Occluded_Human_Mesh_Recovery_CVPR_2022_paper.html" id="OCHMR">Occluded Human Mesh Recovery</a></h3>
                        <div class="authors">Rawal Khirodkar, <strong>Shashank Tripathi</strong>, Kris Kitani</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2022</div>
                        <div class="description">
                            A novel top-down mesh recovery architecture capable of leveraging image spatial
                            context for handling multi-person occlusion and crowding.
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2203.13349" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('ochmr_abs')" class="paper-link">Abstract</a>
                            <a href="https://rawalkhirodkar.github.io/ochmr" class="paper-link">Project</a>
                            <!-- <a href="https://www.https://www.youtube.com/watch?v=Q_43QXR1pzY&t=42s">video</a> -->
                            <!-- <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> | -->
                            <!-- <a href="assets/poster_v5_final_print.pdf">poster</a> -->

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="ochmr_abs" style="display: none;"> Top-down methods
                                    for monocular human mesh recovery have two stages: (1) detect human
                                    bounding boxes; (2) treat each bounding box as an independent
                                    single-human mesh recovery task. Unfortunately, the single-human
                                    assumption does not hold in images with multi-human occlusion and
                                    crowding. Consequently, top-down methods have difficulties in recovering
                                    accurate 3D human meshes under severe person-person occlusion. To
                                    address this, we present Occluded Human Mesh Recovery (OCHMR) - a novel
                                    top-down mesh recovery approach that incorporates image spatial context
                                    to overcome the limitations of the single-human assumption. The approach
                                    is conceptually simple and can be applied to any existing top-down
                                    architecture. Along with the input image, we condition the top-down
                                    model on spatial context from the image in the form of body-center
                                    heatmaps. To reason from the predicted body centermaps, we introduce
                                    Contextual Normalization (CoNorm) blocks to adaptively modulate
                                    intermediate features of the top-down model. The contextual conditioning
                                    helps our model disambiguate between two severely overlapping human
                                    bounding-boxes, making it robust to multi-person occlusion. Compared
                                    with state-of-the-art methods, OCHMR achieves superior performance on
                                    challenging multi-person benchmarks like 3DPW, CrowdPose and OCHuman.
                                    Specifically, our proposed contextual reasoning architecture applied to
                                    the SPIN model with ResNet-50 backbone results in 75.2 PMPJPE on
                                    3DPW-PC, 23.6 AP on CrowdPose and 37.7 AP on OCHuman datasets, a
                                    significant improvement of 6.9 mm, 6.4 AP and 20.8 AP respectively over
                                    the baseline. Code and models will be released. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{khirodkar_ochmr_2022,
title = {Occluded Human Mesh Recovery},
author = {Khirodkar, Rawal and Tripathi, Shashank and Kitani, Kris},
booktitle = {IEEE/CVF Conf.~on Computer Vision and Pattern Recognition
(CVPR)},
month = jun,
year = {2022},
doi = {},
month_numeric = {6}
}
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/agora_teaser.png" alt="AGORA: Avatars in Geography Optimized for Regression Analysis">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://cvml.page.link/agora" id="AGORA">AGORA: Avatars in Geography Optimized for Regression Analysis</a></h3>
                        <div class="authors">Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffman, <strong>Shashank Tripathi</strong> and Michael J. Black</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2021</div>
                        <div class="description">
                            A synthetic dataset with high realism and highly accurate ground truth
                            containing 4240 textured scans and SMPLX fits.
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2104.14643" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('agora_abs')" class="paper-link">Abstract</a>
                            <a href="https://agora.is.tue.mpg.de/" class="paper-link">Project</a>
                            <a
                                href="https://www.https://www.youtube.com/watch?v=Q_43QXR1pzY&t=42s" class="paper-link">Video</a>
                            <!-- <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> | -->
                            <!-- <a href="assets/poster_v5_final_print.pdf">poster</a> -->

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="agora_abs" style="display: none;"> While the accuracy
                                    of 3D human pose estimation from images has steadily improved on
                                    benchmark datasets, the best methods still fail in many real-world
                                    scenarios. This suggests that there is a domain gap between current
                                    datasets and common scenes containing people. To obtain ground-truth 3D
                                    pose, current datasets limit the complexity of clothing, environmental
                                    conditions, number of subjects, and occlusion. Moreover, current
                                    datasets evaluate sparse 3D joint locations corresponding to the major
                                    joints of the body, ignoring the hand pose and the face shape. To
                                    evaluate the current state-of-the-art methods on more challenging
                                    images, and to drive the field to address new problems, we introduce
                                    AGORA, a synthetic dataset with high realism and highly accurate ground
                                    truth. Here we use 4240 commercially-available, high-quality, textured
                                    human scans in diverse poses and natural clothing; this includes 257
                                    scans of children. We create reference 3D poses and body shapes by
                                    fitting the SMPL-X body model (with face and hands) to the 3D scans,
                                    taking into account clothing. We create around 14K training and 3K test
                                    images by rendering between 5 and 15 people per image us- ing either
                                    image-based lighting or rendered 3D environments, taking care to make
                                    the images physically plausible and photoreal. In total, AGORA consists
                                    of 173K individual person crops. We evaluate existing state-of-the- art
                                    methods for 3D human pose estimation on this dataset. and find that most
                                    methods perform poorly on images of children. Hence, we extend the
                                    SMPL-X model to better capture the shape of children. Additionally, we
                                    fine- tune methods on AGORA and show improved performance on both AGORA
                                    and 3DPW, confirming the realism of the dataset. We provide all the
                                    registered 3D reference training data, rendered images, and a web-based
                                    evaluation site at https://agora.is.tue.mpg.de/. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{tripathi2019learning,
title={Learning to generate synthetic data via compositing},
author={Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and
Tyagi, Ambrish
and Rehg, James M and Chari, Visesh},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition},
pages={461--470},
year={2019}
}
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/pose_combined.gif" alt="PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://cvml.page.link/pose" id="PoseNet3D">PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge Distillation</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, <a href="https://www.siddhantranade.com/">Siddhant
                            Ranade</a>, Ambrish
                        Tyagi and Amit Agrawal</div>
                        <div class="venue">International Conference on 3D Vision (3DV), 2020</div>
                        <div class="award">(Oral presentation)</div>
                        <div class="description">
                            Temporally consistent recovery of 3D human pose from 2D joints without using 3D
                            data in any
                            form
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2003.03473" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('posenet3d_abs')" class="paper-link">Abstract</a>
                            <a
                                href="https://www.youtube.com/playlist?list=PL46Tof4i1hsHK6D-y8oBSRK-DpmvfyVlf" class="paper-link">videos</a>
                            <!-- <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> | -->
                            <!-- <a href="assets/poster_v5_final_print.pdf">poster</a> -->

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="posenet3d_abs" style="display: none;"> Recovering 3D
                                    human pose
                                    from 2D joints is a highly unconstrained problem. We propose a novel
                                    neural network
                                    architecture, PoseNet3D, that takes 2D joints as input and outputs 3D
                                    skeletons and SMPL
                                    pose parameters. By casting our learning approach in a Knowledge
                                    Distillation framework,
                                    we avoid using any 3D data such as paired 2D-3D data, unpaired 3D data,
                                    motion capture
                                    sequences or multi-view images during training. We first train a teacher
                                    network that
                                    outputs 3D skeletons, using only 2D poses for training. The teacher
                                    network distills its
                                    knowledge to a student network that predicts 3D pose in SMPL
                                    representation. Finally,
                                    both the teacher and the student networks are jointly fine tuned in an
                                    end-to-end manner
                                    using self-consistency and adversarial losses, improving the accuracy of
                                    the individual
                                    networks. Results on Human3.6M dataset for 3D human pose estimation
                                    demonstrate that our
                                    approach reduces the 3D joint prediction error by 18% or more compared
                                    to previous
                                    methods. Qualitative results show that the recovered 3D poses and meshes
                                    are natural,
                                    realistic, and flow smoothly over consecutive frames. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{tripathi2019learning,
title={Learning to generate synthetic data via compositing},
author={Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and
Tyagi, Ambrish
and Rehg, James M and Chari, Visesh},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition},
pages={461--470},
year={2019}
}
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/terse_teaser.png" alt="Learning to Generate Synthetic Data via Compositing">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://arxiv.org/abs/1904.05475" id="TERSE">Learning to Generate Synthetic Data via Compositing</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, <a href="https://siddharthachandra.github.io/">Siddhartha
                            Chandra</a>, Amit Agrawal, Ambrish Tyagi, <a href="https://rehg.org/"> James Rehg</a> and Visesh
                        Chari</div>
                        <div class="venue">Computer Vision and Pattern Recognition (CVPR) 2019</div>
                        <div class="description">
                            Efficient, task-aware and realisitic synthesis of composite images for training
                            classification and object detection models
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/1904.05475" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('terse_abs')" class="paper-link">Abstract</a>
                            <!--                            <a shape="rect" href="javascript:togglebib(&#39;terse&#39;)" class="togglebib">bibtex</a> |-->
                            <a href="assets/poster_v5_final_print.pdf" class="paper-link">Poster</a>

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="terse_abs" style="display: none;"> We present a
                                    task-aware
                                    approach to synthetic data generation. Our framework employs a trainable
                                    synthesizer
                                    network that is optimized to produce meaningful training samples by
                                    assessing the
                                    strengths and weaknesses of a `target' network. The synthesizer and
                                    target networks are
                                    trained in an adversarial manner wherein each network is updated with a
                                    goal to outdo
                                    the other. Additionally, we ensure the synthesizer generates realistic
                                    data by pairing
                                    it with a discriminator trained on real-world images. Further, to make
                                    the target
                                    classifier invariant to blending artefacts, we introduce these artefacts
                                    to background
                                    regions of the training images so the target does not over-fit to them.
                                    We demonstrate the efficacy of our approach by applying it to different
                                    target networks
                                    including a classification network on AffNIST, and two object detection
                                    networks (SSD,
                                    Faster-RCNN) on different datasets. On the AffNIST benchmark, our
                                    approach is able to
                                    surpass the baseline results with just half the training examples. On
                                    the VOC person
                                    detection benchmark, we show improvements of up to 2.7% as a result of
                                    our data
                                    augmentation. Similarly on the GMU detection benchmark, we report a
                                    performance boost of
                                    3.5% in mAP over the baseline method, outperforming the previous state
                                    of the art
                                    approaches by up to 7.5% on specific categories. </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
    @inproceedings{tripathi2019learning,
    title={Learning to generate synthetic data via compositing},
    author={Tripathi, Shashank and Chandra, Siddhartha and Agrawal, Amit and
    Tyagi, Ambrish
    and Rehg, James M and Chari, Visesh},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition},
    pages={461--470},
    year={2019}
    }
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/c2f_img.PNG" alt="C2F: Coarse-to-Fine Vision Control System for Automated Microassembly">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://www.eurekaselect.com/node/159218/article/c2f-coarse-to-fine-vision-control-system-for-automated-microassembly" id="C2F">C2F: Coarse-to-Fine Vision Control System for Automated Microassembly</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Devesh Jain and Himanshu Dutt Sharma</div>
                        <div class="venue">Nanotechnology and Nanoscience-Asia 2018</div>
                        <div class="description">
                            Automated, visual-servoing based closed loop system to perform 3D
                            micromanipulation and
                            microassembly tasks
                        </div>
                        <div class="paper-links">
                            <a href="https://drive.google.com/file/d/0B0keZAN_TLL9VmZ2dlAxRlYzYjA/view" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('c2f_abs')" class="paper-link">Abstract</a>
                            <a href="https://www.youtube.com/watch?v=lAagBmqj_Nw" class="paper-link">Video</a>


                            <!-- <a shape="rect" href="javascript:togglebib(&#39;c2f&#39;)" class="togglebib">bibtex</a> -->


                            <p align="justify"><i id="c2f_abs" style="display: none;">In this paper, authors
                                    present the
                                    development of a completely automated system to perform 3D
                                    micromanipulation and
                                    microassembly tasks. The microassembly workstation consists of a 3
                                    degree-of-freedom
                                    (DOF) MM3A® micromanipulator arm attached to a microgripper, two 2 DOF
                                    PI® linear
                                    micromotion stages, one optical microscope coupled with a CCD image
                                    sensor, and two CMOS
                                    cameras for coarse vision. The whole control strategy is subdivided into
                                    sequential
                                    vision based routines: manipulator detection and coarse alignment,
                                    autofocus and fine
                                    alignment of microgripper, target object detection, and performing the
                                    required assembly
                                    tasks. A section comparing various objective functions useful in the
                                    autofocusing regime
                                    is included. The control system is built entirely in the image frame,
                                    eliminating the
                                    need for system calibration, hence improving speed of operation. A
                                    micromanipulation
                                    experiment performing pick- and-place of a micromesh is illustrated.
                                    This demonstrates a
                                    three-fold reduction in setup and run time for fundamental
                                    micromanipulation tasks, as
                                    compared to manual operation. Accuracy, repeatability and reliability of
                                    the programmed
                                    system is analyzed.</i></p>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/isbi_image.PNG" alt="Sub-cortical Shape Morphology and Voxel-based Features for Alzheimer's Disease Classification">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950682" id="ALZHEIMERS">Sub-cortical Shape Morphology and Voxel-based Features for Alzheimer's Disease Classification</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Seyed Hossein Nozadi, <a href="https://www.researchgate.net/profile/Mahsa_Shakeri2">Mahsa Shakeri</a>
                        and <a href="http://www.polymtl.ca/expertises/en/kadoury-samuel">Samuel
                            Kadoury</a></div>
                        <div class="venue">IEEE International Symposium on Biomedical
                            Imaging (ISBI) 2017</div>
                        <div class="description">
                            Alzheimer's disease patient classification using a combination of grey-matter
                            voxel-based
                            intensity variations and 3D structural (shape) features extracted from MRI brain scans
                        </div>
                        <div class="paper-links">
                            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7950682" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('sub_cortical_abs')" class="paper-link">Abstract</a>
                            <!--                            <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->
                            <a href="assets/sub_cortical_poster.pdf" class="paper-link">Poster</a>

                            <!-- <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a> -->


                            <p align="justify"><i id="sub_cortical_abs" style="display: none;">
                                    Neurodegenerative
                                    pathologies, such as Alzheimer’s disease, are linked with morphological
                                    alterations and
                                    tissue variations in subcortical structures which can be assessed from
                                    medical imaging
                                    and biological data. In this work, we present an unsupervised framework
                                    for the
                                    classification of Alzheimer’s disease (AD) patients, stratifying
                                    patients into four
                                    diagnostic groups, namely: AD, early Mild Cognitive Impairment (MCI),
                                    late MCI and
                                    normal controls by combining shape and voxel-based features from 12
                                    sub-cortical areas.
                                    An automated anatomical labeling using an atlas-based segmentation
                                    approach is proposed
                                    to extract multiple regions of interest known to be linked with AD
                                    progression. We take
                                    advantage of gray-matter voxel-based intensity variations and structural
                                    alterations
                                    extracted with a spherical harmonics framework to learn the
                                    discriminative features
                                    between multiple diagnostic classes. The proposed method is validated on
                                    600 patients
                                    from the ADNI database by training binary SVM classifiers of
                                    dimensionality reduced
                                    features, using both linear and RBF kernels. Results show near
                                    state-of-the-art
                                    approaches in classification accuracy (>88%), especially for the more
                                    challenging
                                    discrimination tasks: AD vs. LMCI (76.81%), NC vs. EMCI (75.46%) and
                                    EMCI vs. LMCI
                                    (70.95%). By combining multimodality features, this pipeline
                                    demonstrates the potential
                                    by exploiting complementary features to improve cognitive assessment.
                                </i></p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{tripathi2017sub,
title={Sub-cortical shape morphology and voxel-based features for
Alzheimer's disease
classification},
author={Tripathi, Shashank and Nozadi, Seyed Hossein and Shakeri, Mahsa and
Kadoury,
Samuel},
booktitle={Biomedical Imaging (ISBI 2017), 2017 IEEE 14th International
Symposium on},
pages={991--994},
year={2017},
organization={IEEE}
}
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/miccai_img.png" alt="Deep Spectral-Based Shape Features for Alzheimer’s Disease Classification">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://link.springer.com/chapter/10.1007/978-3-319-51237-2_2" id="DEEP_SPECTRAL">Deep Spectral-Based Shape Features for Alzheimer’s Disease Classification</a></h3>
                        <div class="authors"><a href="https://www.researchgate.net/profile/Mahsa_Shakeri2">Mahsa Shakeri</a>,
                            <a href="https://profs.etsmtl.ca/hlombaert/">Hervé Lombaert</a>, <strong>Shashank Tripathi</strong> and
                            <a href="http://www.polymtl.ca/expertises/en/kadoury-samuel">Samuel
                            Kadoury</a></div>
                        <div class="venue">MICCAI Spectral and Shape Analysis
                            in Medical Imaging (SeSAMI) 2016</div>
                        <div class="description">
                            Alzheimer's disease classification by using deep learning variational
                            auto-encoder on shape
                            based features
                        </div>
                        <div class="paper-links">
                            <a href="https://link.springer.com/chapter/10.1007/978-3-319-51237-2_2" class="paper-link">Paper</a>
                            <a href="javascript:toggleblock('deepspectral_abs')" class="paper-link">Abstract</a>
                            <!--                            <a shape="rect" href="javascript:togglebib(&#39;deepspectral&#39;)"-->
                            <!--                               class="togglebib">bibtex</a> |-->
                            <!-- <a href="https://github.com/debidatta/syndata-generation">code</a> |
                            <a href="assets/cutpaste_poster.pdf">poster</a> -->


                            <p align="justify"><i id="deepspectral_abs" style="display: none;">Alzheimer’s
                                    disease (AD)
                                    and mild cognitive impairment (MCI) are the most prevalent
                                    neurodegenerative brain
                                    diseases in elderly population. Recent studies on medical imaging and
                                    biological data
                                    have shown morphological alterations of subcortical structures in
                                    patients with these
                                    pathologies. In this work, we take advantage of these structural
                                    deformations for
                                    classification purposes. First, triangulated surface meshes are
                                    extracted from segmented
                                    hippocampus structures in MRI and point-to-point correspondences are
                                    established among
                                    population of surfaces using a spectral matching method. Then, a deep
                                    learning
                                    variational auto-encoder is applied on the vertex coordinates of the
                                    mesh models to
                                    learn the low dimensional feature representation. A multi-layer
                                    perceptrons using
                                    softmax activation is trained simultaneously to classify Alzheimer’s
                                    patients from
                                    normal subjects. Experiments on ADNI dataset demonstrate the potential
                                    of the proposed
                                    method in classification of normal individuals from early MCI (EMCI),
                                    late MCI (LMCI),
                                    and AD subjects with classification rates outperforming standard SVM
                                    based approach.</i>
                            </p>

                            <div style="white-space: pre-wrap; display: none;" class="bib">
@inproceedings{shakeri2016deep,
title={Deep spectral-based shape features for alzheimer’s disease
classification},
author={Shakeri, Mahsa and Lombaert, Herve and Tripathi, Shashank and
Kadoury, Samuel
and Alzheimer’s Disease Neuroimaging Initiative and others},
booktitle={International Workshop on Spectral and Shape Analysis in Medical
Imaging},
pages={15--24},
year={2016},
organization={Springer}
}
                            </div>
                        </div>
                    </div>
                </article>

            <!-- Patents Section -->
            <section id="patents">
                <h2 class="section-title">Patents</h2>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/3d_pose_patent.png" alt="Three-dimentional Pose Estimation without 3D Data">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://patentimages.storage.googleapis.com/26/18/ce/507e21fc6d642a/US11526697.pdf">Three-dimentional Pose Estimation without 3D Data</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Amit Agrawal, Ambrish Tyagi, Siddhant Ranade</div>
                        <div class="venue">US Patent 11,526,697</div>
                        <div class="description">
                            Patent for unsupervised 3D human pose estimation using geometric consistency.
                        </div>
                        <div class="paper-links">
                            <a href="https://patentimages.storage.googleapis.com/26/18/ce/507e21fc6d642a/US11526697.pdf" class="paper-link">Patent</a>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/3d_patent.png" alt="Generation of synthetic image data using three-dimensional models">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://patents.google.com/patent/US10909349B1">Generation of synthetic image data using three-dimensional models</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, Visesh Chari</div>
                        <div class="venue">US Patent 10,909,349</div>
                        <div class="description">
                            Patent for generating synthetic training data using 3D models and realistic rendering techniques for computer vision applications.
                        </div>
                        <div class="paper-links">
                            <a href="https://patents.google.com/patent/US10909349B1" class="paper-link">Patent</a>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/2d_patent.png" alt="Generation of synthetic image data for computer vision models">
                    </div>
                    <div class="publication-content">
                        <h3><a href="https://patents.google.com/patent/US10860836B1">Generation of synthetic image data for computer vision models</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong>, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M. Rehg, Visesh Chari</div>
                        <div class="venue">US Patent 10,860,836</div>
                        <div class="description">
                            Patent for methods of generating synthetic image data through advanced compositing techniques for training computer vision models.
                        </div>
                        <div class="paper-links">
                            <a href="https://patents.google.com/patent/US10860836B1" class="paper-link">Patent</a>
                        </div>
                    </div>
                </article>
            </section>

            <!-- Miscellaneous Section -->
            <section id="misc">
                <h2 class="section-title">Miscellaneous</h2>
                <p style="margin-bottom: 30px; font-style: italic;">Some other unpublished work:</p>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/supercnn_img.png" alt="Learning Salient Objects in a Scene using Superpixel-augmented Convolutional Neural Networks">
                    </div>
                    <div class="publication-content">
                        <h3><a href="assets/learning_salient_objects.pdf">Learning Salient Objects in a Scene using Superpixel-augmented Convolutional Neural Networks</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong></div>
                        <!-- <div class="venue">Course Project Report</div> -->
                        <div class="description">
                            A method for learning salient object detection using superpixel-augmented convolutional neural networks to improve object localization in complex scenes.
                        </div>
                        <div class="paper-links">
                            <a href="assets/learning_salient_objects.pdf" class="paper-link">Report</a>
                            <a href="assets/SuperCNN.pdf" class="paper-link">Slides</a>
                            <a href="https://github.com/sha2nkt/SuperCNN" class="paper-link">Code</a>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/tracking_combined.gif" alt="Moving object detection, tracking and classification from an unsteady camera">
                    </div>
                    <div class="publication-content">
                        <h3><a href="assets/tracking.pdf">Moving object detection, tracking and classification from an unsteady camera</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong></div>
                        <!-- <div class="venue">Course Project</div> -->
                        <div class="description">
                            A computer vision system for detecting, tracking, and classifying moving objects from unstable camera platforms, addressing challenges in dynamic environments.
                        </div>
                        <div class="paper-links">
                            <a href="assets/tracking.pdf" class="paper-link">Slides</a>
                            <a href="https://www.youtube.com/watch?v=g_nTKhVyPHw" class="paper-link">Video</a>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-image">
                        <img src="assets/model_combined.gif" alt="Towards integrating model dynamics for sample efficient reinforcement learning">
                    </div>
                    <div class="publication-content">
                        <h3><a href="assets/deep-rl-final.pdf">Towards integrating model dynamics for sample efficient reinforcement learning</a></h3>
                        <div class="authors"><strong>Shashank Tripathi</strong></div>
                        <!-- <div class="venue">Course Project Report</div> -->
                        <div class="description">
                            Research on integrating model-based dynamics into reinforcement learning algorithms to improve sample efficiency and learning performance.
                        </div>
                        <div class="paper-links">
                            <a href="assets/deep-rl-final.pdf" class="paper-link">Report</a>
                            <a href="https://github.com/sha2nkt/QD_learning" class="paper-link">Code</a>
                        </div>
                    </div>
                </article>
            </section>
        </div>
    </main>

    <!-- Scroll to Top Button -->
    <button class="scroll-to-top" onclick="scrollToTop()" title="Back to top">
        <i class="fas fa-chevron-up"></i>
    </button>

    <!-- Footer -->
    <footer style="text-align: center; padding: 40px 0; color: var(--text-light); font-size: 0.85rem;">
        <div class="container">
            <p style="margin-top: 15px;">© 2025 Shashank Tripathi. All rights reserved.</p>
        </div>
    </footer>

    <script type="text/javascript">
        hideallbibs();
        hideblock('pa13_abs');
        hideblock('cuboid_abs');
        hideblock('mftcn_abs');

        // Scroll to top functionality
        function scrollToTop() {
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        }
    </script>
</body>

</html>
    <script xml:space="preserve" language="JavaScript">
        hideblock('temporal_abs');
    </script>


</body>

</html>