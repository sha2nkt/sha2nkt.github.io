<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Stolen from Sergey and Jon Barron */
  /* with significant help from Debidatta Dwibedi's webpage */
  a {
    color: #1772d0;
    text-decoration:none;
  }
  a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
  }
  body,td,th {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px
  }
  strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;


    font-size: 14px
  }
  strongred {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    color: 'red'

    font-size: 14px
  }
  heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;*/


    font-size: 15px;
    font-weight: 700
  }
</style>
<!-- <link rel="icon" type="image/png" href="seal_icon.png"> -->
<script type="text/javascript" src="hidebib.js"></script>
<title>Shashank Tripathi</title>
<meta name="Shashank Tripathi&#39;s CMU Homepage" http-equiv="Content-Type" content="Shashank Tripathi&#39;s Homepage">

<link href="css" rel="stylesheet" type="text/css">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90857215-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body>

  <table width="960" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td>
        <p align="center"><font size="7">Shashank Tripathi</font><br>
        </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="67%" valign="middle" align="justify">



              <p>I am a Masters student in <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cmu.edu/">CMU</a>, where I am advised by <a href="http://www.cs.cmu.edu/~kkitani/"> Kris Kitani</a>.
                <br>

              </p><p align="center">
                <a href="mailto:shashank.tripathi123@gmail.com">Email</a> &nbsp;/&nbsp;
                <!-- <a href="assets/DebidattaDwibedi_CV.pdf">CV</a> &nbsp;/&nbsp; -->
                <a href="https://scholar.google.com/citations?user=CANstcsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/sha2nkt"> GitHub</a>&nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/shashanktripathi123/"> LinkedIn </a>
                <!-- <a href="https://www.twitter.com/debidatta/"> Twitter </a> -->
              </p>

              <!--
            </p><p align="center">
              <a href="#publications">Publications</a> &nbsp;/&nbsp;

              <a href="#patents">Patents</a> &nbsp/&nbsp
              <a href="#talks">Talks</a>&nbsp;/&nbsp;
              <a href="#theses">Theses</a>&nbsp;/&nbsp;
              <a href="#misc">Misc</a>
            </p>
              -->


          </td>
          <td width="33%"><img src="assets/shashank.jpg" width="90%" style="background-repeat: no-repeat;background-position: 50%;border-radius: 50%;width: 200px;height: 200px;"></td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



        <tbody><tr><td>
          <h2>Research</h2>

          I want to build intelligent agents that interact with our world in useful ways.
          <br/><br/>
          My research lies at the intersection of machine learning, computer vision and robotics. Presently, I am working on imitation learning from videos.
          <br/><br/>
          In the past, I have worked on human-object interaction recognition in videos, object detection, pose estimation, reinforcement learning, game rule learning and image segmentation.
          
        </td></tr>

      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tbody><tr><td>
          <h2 id="publications">Publications</h2></td></tr>

        <tr>
          <td width="42%" valign="top"><a href="assets/cheetah.gif"><img src="assets/cheetah.gif" width="100%" height="60%" style="border-style: none"></a>
          </td><td width="58%" valign="top">
            <p><a href="https://arxiv.org/abs/1808.00928" id="CHEETAH">
              <heading>Learning  Actionable  Representations  from  Visual  Observations</heading></a><br>
              Debidatta Dwibedi, <a href="https://cims.nyu.edu/~tompson/">Jonathan Tompson</a>, Corey Lynch and <a href="https://sermanet.github.io/home/">Pierre Sermanet</a><br>
              <em><a href="https://www.iros2018.org/">International Conference on Intelligent Robots (IROS) 2018 </a></href> </em>
              <br>
              <br>
              Control agents from pixels by learning self-supervised representations from videos.<br>

            </p>

            <div class="paper" id="mftcn">
              <a href="https://arxiv.org/abs/1808.00928">paper</a> |
              <a href="javascript:toggleblock(&#39;mftcn_abs&#39;)">abstract</a> |
              <a shape="rect" href="javascript:togglebib(&#39;mftcn&#39;)" class="togglebib">bibtex</a> |
              <a href="https://sites.google.com/corp/view/actionablerepresentations/"> project </a>


              <p align="justify"> <i id="mftcn_abs" style="display: none;"> In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline.</i> </p>

              <div style="white-space: pre-wrap; display: none;" class="bib">
                @article{dwibedi2018learning,
                title={Learning Actionable Representations from Visual Observations},
                author={Dwibedi, Debidatta and Tompson, Jonathan and Lynch, Corey and Sermanet, Pierre},
                journal={arXiv preprint arXiv:1808.00928},
                year={2018}
              }
            </div>
          </div>
        </td>
      </tr>

      <tr>
        <td width="42%" valign="top"><a href="https://arxiv.org/abs/1708.01642"><img src="assets/cutpaste.png" width="100%" style="border-style: none"></a>
        </td><td width="58%" valign="top">
          <p><a href="https://arxiv.org/abs/1708.01642" id="CUTPASTE">
            <heading>Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</heading></a><br>
            Debidatta Dwibedi, <a href="http://imisra.github.io/">Ishan Misra</a> and <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a><br>
            <em><a href="http://iccv2017.thecvf.com/">International Conference on Computrer Vision (ICCV) 2017</a></em>
            <br>
            <br>
            Generate synthetic data for detecting objects in scenes.<br>

          </p>

          <div class="paper" id="cutpaste">
            <a href="https://arxiv.org/abs/1708.01642">paper</a> |
            <a href="javascript:toggleblock(&#39;cutpaste_abs&#39;)">abstract</a> |
            <a shape="rect" href="javascript:togglebib(&#39;cutpaste&#39;)" class="togglebib">bibtex</a> |
            <a href="https://github.com/debidatta/syndata-generation">code</a> |
            <a href="assets/cutpaste_poster.pdf">poster</a>


            <p align="justify"> <i id="cutpaste_abs" style="display: none;">A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10% real data outperforms models trained on all real data.</i> </p>

            <div style="white-space: pre-wrap; display: none;" class="bib">
              @InProceedings{Dwibedi_2017_ICCV,
              author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
              title = {Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection},
              booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
              month = {Oct},
              year = {2017}
            }
          </div>
        </div>
      </td>
    </tr>

    <tr>
      <td width="42%" valign="top"><a href="assets/cuboid.png"><img src="assets/cuboid.png" width="100%" style="border-style: none"></a>
      </td><td width="58%" valign="top">
        <p><a href="https://arxiv.org/abs/1611.10010v1" id="CUBOID">
          <heading>Deep Cuboid Detection: Beyond 2D Bounding Boxes</heading></a><br>
          Debidatta Dwibedi, <a href="http://people.csail.mit.edu/tomasz/">Tomasz Malisiewicz</a>, <a href="https://sites.google.com/site/vijaybacademichomepage/home">Vijay Badrinarayanan</a> and <a href="https://ai.google/research/people/AndrewRabinovich">Andrew Rabinovich</a><br>
          <em>Arxiv Preprint</em>, 2016
          <br>
          <br>
          Cuboid detector using deep learning: finds cuboids in scenes and localizes their corners.<br>

        </p>

        <div class="paper" id="cuboid">
          <a href="https://arxiv.org/abs/1611.10010v1">paper</a> |
          <a href="javascript:toggleblock(&#39;cuboid_abs&#39;)">abstract</a> |
          <a shape="rect" href="javascript:togglebib(&#39;cuboid&#39;)" class="togglebib">bibtex</a>


          <p align="justify"> <i id="cuboid_abs" style="display: none;">We present a Deep Cuboid Detector which takes a consumer-quality RGB image of a cluttered scene and localizes all 3D cuboids (box-like objects). Contrary to classical approaches which fit a 3D model from low-level cues like corners, edges, and vanishing points, we propose an end-to-end deep learning system to detect cuboids across many semantic categories (e.g., ovens, shipping boxes, and furniture). We localize cuboids with a 2D bounding box, and simultaneously localize the cuboid's corners, effectively producing a 3D interpretation of box-like objects. We refine keypoints by pooling convolutional features iteratively, improving the baseline method significantly. Our deep learning cuboid detector is trained in an end-to-end fashion and is suitable for real-time applications in augmented reality (AR) and robotics.</i> </p>

          <div style="white-space: pre-wrap; display: none;" class="bib">
            @article{dwibedi2016deep,
            title={Deep cuboid detection: Beyond 2d bounding boxes},
            author={Dwibedi, Debidatta and Malisiewicz, Tomasz and Badrinarayanan, Vijay and Rabinovich, Andrew},
            journal={arXiv preprint arXiv:1611.10010},
            year={2016}
          }
        </div>
      </div>
    </td>
  </tr>

  <tr>
    <td width="42%" valign="top"><a href="assets/semantic_graph.png"><img src="assets/semantic_graph.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="top">
      <p><a href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_23" id="PA13">
        <heading>Characterizing Predicate Arity and Spatial Structure for Inductive Learning of Game Rules</heading></a><br>
        Debidatta Dwibedi and <a href="http://www.cse.iitk.ac.in/users/amit/">Amitabha Mukerjee</a><br>
        <em><a href="http://profs.sci.univr.it/~cristanm/contact2014/">ECCV 2014 Workshop on Computer Vision + Ontology Applied Cross-Disciplinary Technologies 2014</a></em>
        <br>
        <br>
        Represent videos as dynamic graphs. Learn rules of games from observing people play games in Kinect videos.<br>

      </p>

      <div class="paper" id="pa13">
        <a href="http://link.springer.com/chapter/10.1007/978-3-319-16181-5_23">paper</a> |
        <a href="javascript:toggleblock(&#39;pa13_abs&#39;)">abstract</a> |
        <a shape="rect" href="javascript:togglebib(&#39;pa13&#39;)" class="togglebib">bibtex</a> |
        <a href="https://www.youtube.com/playlist?list=PLDd4RddCgh3ctcftujGoS6CzC47fipqWt">videos</a>


        <p align="justify"> <i id="pa13_abs" style="display: none;">Where do the predicates in a game ontology come from? We use RGBD vision to learn a) the spatial structure of a board, and b) the number of parameters in a move or transition. These are used to define state-transition predicates for a logical description of each game state. Given a set of videos for a game, we use an improved 3D multi-object tracking to obtain the positions of each piece in games such as 4-peg solitaire or Towers of Hanoi. The spatial positions occupied by pieces over the entire game is clustered, revealing the structure of the board. Each frame is represented as a Semantic Graph with edges encoding spatial relations between pieces. Changes in the graphs between game states reveal the structure of a “move”. Knowledge from spatial structure and semantic graphs is mapped to FOL descriptions of the moves and used in an Inductive Logic framework to infer the valid moves and other rules of the game. Discovered predicate structures and induced rules are demonstrated for several games with varying board layouts and move structures.</i> </p>

        <div style="white-space: pre-wrap; display: none;" class="bib">
          @inproceedings{dwibedi2014characterizing,
          title={Characterizing Predicate Arity and Spatial Structure for Inductive Learning of Game Rules},
          author={Dwibedi, Debidatta and Mukerjee, Amitabha},
          booktitle={European Conference on Computer Vision},
          pages={323--338},
          year={2014},
          organization={Springer}
        }
      </div>
    </div>
  </td>
</tr>



</tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="patents">Patents</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="https://patents.google.com/patent/US20180137642A1/en"><img src="assets/cuboid2.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="https://patents.google.com/patent/US20180137642A1/en"><heading>Deep learning system for cuboid detection</heading></a><br>
      </p>

    </td>
  </tr>

</tbody></table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="talks">Talks</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/bivu2018.pdf"><img src="assets/bivu_teaser.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/bivu2018.pdf"><heading>Temporal Reasoning in Videos Using Convolutional Gated Recurrent Units</heading></a><br>
        <em><a href="https://bivu2018.github.io/#program">2nd Workshop in Brave New Ideas in Video Understanding</a> at <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a></em>
      </p>
      <div class="paper" id="temporal_ws">
        <a href="assets/bivu2018.pdf">paper</a> | <a href="assets/bnivu_slides.pdf">slides</a> | <a href="assets/bnivu_poster.pdf">poster</a> | <a shape="rect" href="javascript:togglebib(&#39;temporal_ws&#39;)" class="togglebib">bibtex</a>

        <div style="white-space: pre-wrap; display: none;" class="bib">
          @InProceedings{Dwibedi_2018_CVPR_Workshops,
          author = {Dwibedi, Debidatta and Sermanet, Pierre and Tompson, Jonathan},
          title = {Temporal Reasoning in Videos Using Convolutional Gated Recurrent Units},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
          month = {June},
          year = {2018}
        }

      </div>

    </div>
  </td>
</tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/mftcn_mlpc_paper.pdf"><img src="assets/mftcn_sampling.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/mftcn_mlpc_paper.pdf"><heading>Self-Supervised Representation Learning for Continuous Control</heading></a><br>
        <em><a href="http://www.cs.unm.edu/amprg/Workshops/MLPC18/schedule.html">3rd Workshop in Machine Learning in the Planning and Control of Robot Motion</a> at <a href="https://icra2018.org/">ICRA 2018</a></em>
      </p>
      <div class="paper" id="mlpc">
        <a href="assets/mftcn_mlpc_paper.pdf">paper</a> | <a href="assets/mftcn_mlpc_slides.pdf">slides</a> | <a href="assets/mftcn_mlpc_poster.pdf">poster</a>

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="theses">Theses</h2>
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/ms_thesis.pdf"><img src="assets/ms_teaser.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/ms_thesis.pdf"><heading>Synthesizing Scenes for Instance Detection</heading></a><br>
        How can we create annotated datasets without humans for tasks like object detection and pose estimation?
      </p>
      <div class="paper" id="ms">
        <a href="assets/ms_thesis.pdf">thesis</a> | <a href="assets/ms_slides.pdf">slides</a> 

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/mtech_thesis.pdf"><img src="assets/gamerules.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/mtech_thesis.pdf"><heading>Observational  Learning  of Rules  of  Games</heading></a><br>
        Can we learn the rules of a game by observing people playing them?
      </p>
      <div class="paper" id="mtech">
        <a href="assets/mtech_thesis.pdf">thesis</a> | <a href="assets/mtech_slides.pdf">slides</a> 

      </div>
    </td>
  </tr>    
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <h2 id="misc">Miscellaneous</h2>
    Some other unpublished work:
  </td></tr>
</tbody></table>

<table width="100%" align="center" border="0" cellpadding="20">
  <tbody><tr>
    <td width="42%" valign="top"><a href="assets/drl.png"><img src="assets/drl.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/10701_final.pdf"><heading>Playing Games with Deep Reinforcement Learning</heading></a><br>
      </p>
      <div class="paper" id="mlpc">
        <a href="assets/10701_final.pdf"> report </a> | 
        <a href="https://www.youtube.com/watch?v=TqIUM7qlDC0">video</a> 

      </div>

    </td>
  </tr>
  <tr>
    <td width="42%" valign="top"><a href="assets/pose.png"><img src="assets/pose.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/16811_project.pdf"><heading>Towards Pose Estimation of 3D Objects in Monocular Images via Keypoint Detection</heading></a><br>
      </p>

    </td>
  </tr>    

  <tr>
    <td width="42%" valign="top"><a href="assets/handnet.png"><img src="assets/handnet.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/handnet.png"><heading>HandNet: Using Faster R-CNN to Detect Hands in Egocentric Videos</heading></a><br>
      </p>

    </td>
  </tr>   

  <tr>
    <td width="42%" valign="top"><a href="https://www.youtube.com/watch?v=VKo1L9OzB2c"><img src="assets/nao.png" width="100%" style="border-style: none"></a>
    </td><td width="58%" valign="center">
      <p>
        <a href="assets/se367report.pdf"><heading>A Grounded Framework for Gestures and its Applications</heading></a><br>
      </p>

    </td>
  </tr>    

</tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
    <br>
    <p align="right"><font size="2">
      <a href="https://www.cs.berkeley.edu/~barron/">this guy's webpage is awesome</a>
    </font></p>

  </td></tr>


</tbody></table>

</td>
</tr>
</tbody></table>

<script xml:space="preserve" language="JavaScript">
  hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pa13_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('cuboid_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mftcn_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('temporal_abs');
</script>


</body></html>